{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiDAR Basics with VLP-16\n",
    "\n",
    "LIDAR—Light Detection and Ranging—is used to find the precise distance of objects in relation to us. Velodyne LiDAR sensors use time-of-flight methodology for doing this.\n",
    "\n",
    "When a laser pulse is emited, its time-of-shooting and direction are registered. The laser pulse travels through air until it hits an obstacle which reflects some of the energy. The time-of-acquisition and power received are registered by sensor after recieving the portion of energy. The spherical coordinates of the obstacle is calculated using time-of-acquisition which is returned along with power received(as relectance) after each scan.\n",
    "\n",
    "As LiDAR sensor returns reading in spherical coordinates, let's brush up with the spherical coordinate system.\n",
    "___\n",
    "To know more on LiDAR's history and how it works this [blog](https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff) by Oliver Cameron is helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spherical Coordinate System\n",
    "In a spherical coordinate system, a point is defined by a distance and two angles. To represent the two angles we use azimuth($\\theta$) and polar angle($\\phi$) convention. Thus a point is defined by $(\\text{r}, \\theta, \\phi)$.\n",
    "\n",
    "![](resources/lidar_basics/3D_Spherical.svg \"Spherical coordinates (r, θ, φ): radial distance r, polar angle θ (theta), and azimuthal angle φ (phi). The symbol ρ (rho) is often used instead of r. This diagram is by Andeggs from Wikimedia Commons.\")\n",
    "\n",
    "As you can see from above diagram, the azimuth angle is in X-Y plane measured from X-axis and polar angle is in Z-Y plane measured from Z axis.\n",
    "\n",
    "From above diagram, we can get the following equations for converting a cartesian coordinate to spherical coordinates.\n",
    "\n",
    "<math>\\begin{align} r&=\\sqrt{x^2 + y^2 + z^2} \\\\ \\theta &= \\arccos\\frac{z}{\\sqrt{x^2 + y^2 + z^2}} = \\arccos\\frac{z}{r} \\\\ \\varphi &= \\arctan \\frac{y}{x} \\end{align}</math>\n",
    "\n",
    "We can derive cartesian coordinates from spherical coordinates using below equations.\n",
    "\n",
    "<math>\\begin{align} x&=r \\, \\sin\\theta \\, \\cos\\varphi \\\\ y&=r \\, \\sin\\theta \\, \\sin\\varphi \\\\ z&=r \\, \\cos\\theta\\end{align}</math>\n",
    "\n",
    "___\n",
    "Read [more](https://en.wikipedia.org/wiki/Spherical_coordinate_system) about Spherical coordinate system at Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLP-16 coordinate system\n",
    "Velodyne VLP-16 returns reading in spherical coordinates. But there is a slight difference from the above discussed convention.\n",
    "\n",
    "In sensor coordinate system, a point is defined by (radius $\\text{r}$, elevation $\\omega$, azimuth $\\alpha$). Elevation angle, $\\omega$ is in Z-Y plane measured from Y-axis. Azimuth angle, $\\alpha$ is in X-Y plane measured from Y-axis. Below is the diagram.\n",
    "\n",
    "![](resources/lidar_basics/vlp_16_coordinate_sys.svg)\n",
    "\n",
    "Cartesian coordinates can be derived by following equations.\n",
    "\n",
    "<math>\\begin{align} x&=r \\, \\cos\\omega \\, \\sin\\alpha \\\\ y&=r \\, \\cos\\omega \\, \\cos\\alpha \\\\ z&=r \\, \\sin\\omega\\end{align}</math>\n",
    "\n",
    "A computation is necessary to convert the spherical data from the sensor to Cartesian coordinates using above equations. This can be done using a ros package.\n",
    "\n",
    "![](resources/lidar_basics/Lidar_on_car_coord.svg)\n",
    "Above diagram shows the coordinate system of sensor mounted on a car.\n",
    "___\n",
    "This [manual](https://velodynelidar.com/docs/manuals/63-9243%20REV%20D%20MANUAL,USERS,VLP-16.pdf) is a good start to know more about Velodyne VLP-16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "An unstructured point cloud is returned after each scan by sensor. Even though LiDAR returns reading in spherical coordinates, widely used coordinate system is Cartesian.\n",
    "\n",
    "A point in point cloud is defined by it's coordinates and reflectance. Reflectance tells us the reflectivity of the surface. A zero value in reflectance denotes that the laser pulse didn't result in a measurement.\n",
    "\n",
    "There are many formats to store and process point clouds like PCD, PCL but we can treat point cloud as a `Numpy` array. Each element of the array will have `(x,y,z,r)`. For processing point cloud we can use `Numpy`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
